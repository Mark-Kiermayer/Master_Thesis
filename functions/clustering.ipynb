{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Clustering related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of members for each cluster\n",
    "def kmeans_counts(data_labels, clusters =100):\n",
    "    \n",
    "    counts = np.zeros(shape = (clusters,1))\n",
    "    for i in range(clusters):\n",
    "        counts[i] = (data_labels == i).sum()\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Use labeling of k-Means Clustering and optimize the corresponding representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This Procedure combines the functions 'get_representatives_model', 'get_representative_configuration' and\n",
    "# 'get_representatives_train', which operate all on a single model basis\n",
    "# Now we combine the procedures for the list of models for all clusters\n",
    "# Automize Procedure to obtain representatives of pre-determined clusters\n",
    "# Input: List of data for given number of clusters.\n",
    "# Optional input: loss_type, patience of early stopping (es_patience), model name (for saving the best model), epochs\n",
    "#                optimizer, learning rate of optimizer, decay of optimizer\n",
    "# Output: list of representatives and list of models\n",
    "\n",
    "def cluster_ann(y_lst, model_pretrained, N_ensembles =5, qualitative_option = False, \n",
    "                optimizer = 'adam', loss_type = 'mse', metric_type = 'mae',\n",
    "                N_epochs = 4000, N_data = 100, es_patience = 15, \n",
    "                option_pretrain = False, optimizer_pretrain = 'adam',\n",
    "                option_callbacks = 'all',\n",
    "        wd_cluster=r'C:\\Users\\mark.kiermayer\\Documents\\Python Scripts\\Master Thesis - Code\\checkpoints\\Cluster'):\n",
    "    \n",
    "    t_start = time.time()\n",
    "    # Parameters\n",
    "    N_clusters = len(y_lst)\n",
    "    N_features = 4\n",
    "    representatives = np.zeros(shape=(N_clusters,N_features))\n",
    "    representatives_pv = np.zeros(shape=(N_clusters,y_lst[0].shape[1]))\n",
    "    model_weights = []\n",
    "    history = []\n",
    "    times = np.zeros(N_clusters)\n",
    "    \n",
    "    # General Config\n",
    "    constraint_opt = False\n",
    "    midlayer = False\n",
    "    \n",
    "    # Get general model (for all clusters)\n",
    "    model_clustering = get_representatives_model(N_input=N_features, scale=V_max, N_ensembles=N_ensembles, \n",
    "                                                   mid_layer_option = midlayer,\n",
    "                                                   constraint_option=constraint_opt, \n",
    "                                                   qualitative_option = qualitative_option, version = 'v2')\n",
    "    \n",
    "    #Get initial weights, to reset them for every cluster\n",
    "    # Only the weights between input layer and 1st hidden layer are trainable/ will change thru-out training\n",
    "    w_clustering_init = model_clustering.layers[1].get_weights()\n",
    "    \n",
    "    # Import weights for model (Note: Including a constraint influences the number of layers)\n",
    "    get_representative_configuration(model_update = model_clustering, model_weights= model_pretrained, \n",
    "                                         layer_start = 3)  \n",
    "    \n",
    "    \n",
    "    # Function to obtain representative contracts (i.e. hidden output)\n",
    "    get_cluster_representative = K.function(inputs = [model_clustering.layers[0].input],\n",
    "                                  outputs=[model_clustering.layers[1].output])\n",
    "    \n",
    "    print('Model set up. Time required: ' + str(np.round_(time.time()-t_start, decimals = 2))+ ' sec.')\n",
    "    \n",
    "    for i in range(N_clusters):\n",
    "        if i>0:\n",
    "            # reset (trainable) weights\n",
    "            model_clustering.layers[1].set_weights(w_clustering_init)\n",
    "            \n",
    "        y = y_lst[i]\n",
    "        t_start = time.time()\n",
    "        print('Model for Cluster {} of {}'.format(i+1,len(y_lst)))        \n",
    "        print('\\t Training in progress')\n",
    "        \n",
    "        if option_pretrain:\n",
    "            # Optional Pretraining (e.g. with Adam optimizer)\n",
    "            get_representatives_train(model = model_clustering, x = None, y = y, \n",
    "                                      optimizer= optimizer_pretrain, loss = loss_type, metric = metric_type,\n",
    "                                      N_epochs = N_epochs, es_patience = es_patience, cluster_number = str(i), \n",
    "                                      N_x = N_data, option_callbacks = option_callbacks,\n",
    "                                      wd_cluster=wd_cluster)\n",
    "        \n",
    "        # Train model (e.g. mit AdaDelta optimizer)\n",
    "        hist = get_representatives_train(model = model_clustering, x = None, y = y, \n",
    "                                  optimizer= optimizer, loss = loss_type, metric = metric_type,\n",
    "                                  N_epochs = N_epochs, es_patience = es_patience, cluster_number = str(i), \n",
    "                                  N_x = N_data,wd_cluster=wd_cluster)\n",
    "        history.append(hist.history)\n",
    "        times[i] = np.round_(time.time()-t_start, decimals = 2)\n",
    "        print(' \\t Cluster {} completed. Time passed '.format(i+1)+ str(times[i])+ ' sec.')\n",
    "        \n",
    "        # Save weights of trainable layer -> model can be reconstructed later on\n",
    "        model_weights.append(model_clustering.layers[1].get_weights())\n",
    "        \n",
    "        # Save representative (i.e. hidden output)\n",
    "        representatives[i,:] = get_cluster_representative([np.array([1,1,1,1]).reshape((1,4))])[0]\n",
    "        # Save corresponding reserve\n",
    "        representatives_pv[i,:] = model_clustering.predict(x = np.array([1,1,1,1]).reshape((1,4)))[0]\n",
    "    \n",
    "    return [representatives, representatives_pv, model_weights, times, history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model for the determination of a cluster's representative\n",
    "# Procedure will have to be repeated for each cluster\n",
    "# Input: N number of members of cluster, scale: scaling factor in Ensemble model\n",
    "# Output: Model\n",
    "# Options: mid_layer_option: include layer between flattened input and dense layer with \n",
    "#          _features units, to bundle information\n",
    "#         contraint_option: Include unit_constraint (sum of exiting weights for all neuron <1) in all layers\n",
    "#         qualitative_option: Include qualitative model\n",
    "\n",
    "def get_representatives_model(N_input=None, N_features = 4, scale=1,  version = 'v1', N_repeat = 41, N_output = 41, \n",
    "                              N_ensembles = 1, constraint_option = False, mid_layer_option = True,\n",
    "                              qualitative_option = False):\n",
    "    \n",
    "    count = 0\n",
    "    if version == 'v1':\n",
    "        # Version v1: Use all members of (by kMeans) predefined cluster to create a representative contract\n",
    "        INPUT = Input(shape = (N_input,N_features))\n",
    "        count +=1\n",
    "        x = Flatten()(INPUT)\n",
    "        count +=1\n",
    "        if mid_layer_option:\n",
    "            if constraint_option:\n",
    "                x = Dense(units = max(int((N*N_features)/10), 4), activation = 'linear', \n",
    "                          kernel_constraint =keras.constraints.MinMaxNorm(min_value=-1.0,max_value=1.0,rate=0.8,axis=0),\n",
    "                          #keras.constraints.UnitNorm(axis=0), \n",
    "                          bias_constraint = keras.constraints.MinMaxNorm(min_value=-1.0, max_value=1.0, rate=.8, axis=0))(x)\n",
    "                          #keras.constraints.UnitNorm(axis=0))\n",
    "                count +=1          \n",
    "                x = Activation(activation='tanh')(x)\n",
    "                count +=1\n",
    "            else:\n",
    "                x = Dense(units = max(int((N*N_features)/10), 4), activation = 'tanh')(x)\n",
    "                count +=1\n",
    "                \n",
    "        if constraint_option:\n",
    "            x = Dense(units = N_features, activation = 'linear', \n",
    "                      kernel_constraint = keras.constraints.MinMaxNorm(min_value=-1.0,max_value=1.0,rate=0.8,axis=0),\n",
    "                      #keras.constraints.UnitNorm(axis=0), \n",
    "                      bias_constraint = keras.constraints.MinMaxNorm(min_value=-1.0,max_value=1.0,rate=0.8,axis=0))(x)\n",
    "                      #keras.constraints.UnitNorm(axis=0))\n",
    "            count +=1\n",
    "            x = Activation(activation = 'tanh')(x)\n",
    "            count +=1\n",
    "        else:\n",
    "            x = Dense(units = N_features, activation = 'tanh')(x)\n",
    "            count +=1\n",
    "            \n",
    "    elif version == 'v2':\n",
    "        # Version v2: Only the cummulative reserve of the cluster is relevant for the model, not the members' features\n",
    "        # Model will find a representative contract to optimally fit the target (with no respect to the input contracts)\n",
    "        INPUT = Input(shape = (N_features,))\n",
    "        count +=1\n",
    "        x = Dense(units = N_features, activation = 'tanh')(INPUT)\n",
    "        count +=1\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print('Version of model unknown!')\n",
    "        pass\n",
    "    \n",
    "    # Proceed with Agglomerated information (regardless whether model version v1 or v2) \n",
    "    # and use non-trainable pretrained model\n",
    "    \n",
    "    # Note: This is contrary to the previous data preparation\n",
    "    # Here: No default value input when contract matured\n",
    "    x = RepeatVector(n = N_repeat)(x) \n",
    "    count +=1\n",
    "    #x = Lambda(lambda x_val: data_full_transform_test(x_val, n_aim = N_output, input_type = 'scaled', Max_min = Max_min))\n",
    "\n",
    "    if N_ensembles >1:\n",
    "        # include previous choice of Ensemble Model\n",
    "        OUTPUT = combine_models(input_layer = x, n_ensembles =N_ensembles,\n",
    "                                model_qualitative_option = qualitative_option, \n",
    "                                model_qualitative_type = 'plain',\n",
    "                               load_weights = False, weights_ensembles = None, \n",
    "                               weights_qualitative = None, scale = scale, LSTM_nodes = [N_output], \n",
    "                                output_nodes = N_output,\n",
    "                                  final_dense_layer = True, dense_act_fct = 'tanh', act_fct_special = False, \n",
    "                                return_option = 'output')\n",
    "        \n",
    "    else: # For a 1-Model-Ensemble the average-Layer cannot be evaluated\n",
    "        OUTPUT = create_rnn_model(model_input=x, nodes=[N_output],  n_output=N_output, final_dense_layer = True, \n",
    "                     dense_act_fct = 'tanh', act_fct_special = False,\n",
    "                     optimizer_type='adam', loss_type='mse', metric_type='mae', dropout_option=False, \n",
    "                     dropout_share=[0.2,0.2], lambda_layer = True, lambda_scale =scale, log_scale=True, \n",
    "                     model_compile = True, return_option = 'output', branch_name = '', input_type = '3D')\n",
    "        \n",
    "        \n",
    "        \n",
    "    model = Model(inputs = INPUT, outputs = OUTPUT)\n",
    "\n",
    "    for i in range(count,len(model.layers)):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer pretrained weights to model and set corresponding parts as non-trainable\n",
    "# Input: weights\n",
    "def get_representative_configuration(model_update, model_weights, layer_start = 5, optimizer = 'adam', \n",
    "                                     loss = 'mse', metric = 'mae'):\n",
    "    \n",
    "    n = len(model_weights.layers)\n",
    "    # Check if model suitable\n",
    "    if(len(model_update.layers) != n+layer_start-1):\n",
    "        print('Inputs are not compatible!')\n",
    "        return\n",
    "    \n",
    "    # Compile model\n",
    "    # Note: Every time you compile, the weights are reset to default, random initialization\n",
    "    model_update.compile(optimizer = optimizer, loss = loss, metrics = [metric] )\n",
    "       \n",
    "    # If model suitable, continue with layer-wise weights-transfer\n",
    "    for i in range(n-1):\n",
    "        # For model to update: skip 4 Layers to get to Ensemble Part\n",
    "        # For Ensemble Model: skip 1 Layer (i.e. Input Layer)\n",
    "        model_update.layers[layer_start+i].set_weights(model_weights.layers[1+i].get_weights())\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a single model of the list of clustering models\n",
    "def get_representatives_train(model,y,x= None, N_epochs = 2000, optimizer = 'adam', loss = 'mse', metric = 'mae',\n",
    "                              es_patience = 500, cluster_number = None, show_progress = False,\n",
    "                              option_callbacks = 'es', N_x = 1,\n",
    "        wd_cluster = r'C:\\Users\\mark.kiermayer\\Documents\\Python Scripts\\Master Thesis - Code\\checkpoints\\Cluster'):\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = [metric])\n",
    "    \n",
    "    if x == None:\n",
    "        # Create input (1,1,1,1) and repeat targets accordingly\n",
    "        x = np.tile(A = [1,1,1,1], reps = N_x).reshape((N_x, 4))\n",
    "        y = np.tile(y, reps = N_x).reshape((N_x, y.shape[1]))\n",
    "    \n",
    "    if (option_callbacks == 'all') |(option_callbacks == 'es')|(option_callbacks == 'mc'):\n",
    "        if (option_callbacks == 'all')|(option_callbacks == 'es'):\n",
    "            # patient early stopping\n",
    "            es = EarlyStopping(monitor='loss', mode='min', verbose=show_progress, patience=es_patience)\n",
    "        if (option_callbacks == 'all')|(option_callbacks == 'mc'):\n",
    "            # save only best configuration\n",
    "            mc = ModelCheckpoint(wd_cluster+r'\\best_model_cluster_{}.h5'.format(cluster_number), monitor='loss', \n",
    "                                 mode='min', verbose=show_progress, save_best_only=True)\n",
    "        # Fit model\n",
    "        if (option_callbacks == 'mc'):\n",
    "            hist = model.fit(x,y,batch_size=1, epochs = N_epochs, callbacks= [mc], verbose = show_progress)\n",
    "        elif (option_callbacks == 'es'):\n",
    "            hist = model.fit(x,y,batch_size=1, epochs = N_epochs, callbacks= [es], verbose = show_progress)\n",
    "        else:\n",
    "            hist = model.fit(x,y,batch_size=1, epochs = N_epochs, callbacks= [es,mc], verbose = show_progress)     \n",
    "    else:\n",
    "        hist = model.fit(x,y,batch_size=1, epochs = N_epochs, verbose = show_progress)  \n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Utilize concept of self organizing feature maps to obtain (alternative) clustering\n",
    "### (Not fruitful so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Competitive Clustering ANN, \n",
    "## with integrated reserve-prediction-ensemble model (pretrained and non-trainable) and supervised by reserve profile\n",
    "## Option 1: Output Features (Age,Sum ins, Duration, Age of Contract)\n",
    "## Option 2: Output Reserve, i.e. Use Model from option 1 and add the model for predicting reserves as configured in Part I\n",
    "def create_clustering_competitive(INPUT, N_clusters, PV_max = 1, N_features = 4, N_timesteps_ensemble = 41, \n",
    "                                  N_competitive_repeat = 20, dropout_option = True, output_type = 'reserve'):\n",
    "    \n",
    "    #\n",
    "    if (output_type != 'reserve') & (output_type != 'feature'):\n",
    "        print('unknown output_type.')\n",
    "        return\n",
    "    \n",
    "    x = Dense(units=N_clusters, use_bias= True, activation = 'linear')(INPUT)\n",
    "    if droptout_option:\n",
    "        x = Dropout(rate = 0.2)(x)\n",
    "    x = Dense(units=N_clusters, activation = 'relu')(x)\n",
    "    if droptout_option:\n",
    "        x = Dropout(rate = 0.2)(x)\n",
    "    \n",
    "    # Version 1: Use Lambda-Layer to implement Competitive Layer \n",
    "    # Important: Set Layer as non-trainable lateron\n",
    "    #x = Lambda(lambda x_val: np.eye(N_clusters, -K.argmax(x, axis = 0)))(x)\n",
    "    #np.eye(N = N_clusters, M = None, k = -K.argmax(x_val, axis = 0)))(x)\n",
    "    \n",
    "    # Version 2: implement competitive layer using RNN\n",
    "    x = RepeatVector(N_competitive_repeat, name = 'RNN_Data_Prep_0')(x)\n",
    "    x = SimpleRNN(units = N_clusters, use_bias=False, name = 'Competitive_Cluster_Layer', activation = 'relu')(x)\n",
    "    x = Activation(activation='softmax')(x)\n",
    "    \n",
    "    # ensure that each node of this output is in [-1,+1] -> cluster representative features are explicitely given!\n",
    "    x = Dense(units = N_features, activation = 'tanh', name = 'Representative_Features')(x) \n",
    "    \n",
    "    if output_type == 'feature':\n",
    "        model = Model(inputs = INPUT, outputs = x)\n",
    "        return model\n",
    "    else:\n",
    "    \n",
    "        x = RepeatVector(N_timesteps_ensemble, name = 'RNN_Data_Prep_1')(x)\n",
    "\n",
    "        #x = combine_models_test(x, n_ensembles =1, model_qualitative_option = False, \n",
    "        #                   input_layer_qualitative = None, load_weights = False, weights_ensembles = None, \n",
    "        #                   weights_qualitative = None, scale = PV_max, LSTM_nodes = [41], output_nodes = 41,\n",
    "        #                   final_dense_layer = True, dense_act_fct = 'tanh', act_fct_special = False, return_option = 'output')\n",
    "\n",
    "        # x value will be used for quantative part to -> use y\n",
    "        y = create_multiple_rnn_models(5, x,nodes=[41],  n_output=41, final_dense_layer = True, \n",
    "                                   dense_act_fct = 'tanh', optimizer_type='adam', loss_type='mse', \n",
    "                                   metric_type='mae', dropout_option=False, \n",
    "                                   lambda_layer = True, lambda_scale =PV_max, log_scale=False, model_compile = True, \n",
    "                                   return_option = 'output')\n",
    "        y = Average(name = 'Create_Ensemble')(y)\n",
    "\n",
    "        z = Lambda(lambda x_val: x_val[:,:,2:4], name = 'Extract_Info')(x)\n",
    "\n",
    "        z = create_model_qualitative(z, return_option = 'output')\n",
    "        z = Lambda(lambda x_val: tf.round(x_val), name = 'Binary_Transformation')(z)\n",
    "        OUTPUT = multiply([y,z], name = 'Combine_Models')\n",
    "        model = Model(inputs = INPUT, outputs = OUTPUT)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer the pre-trained weight configuration of the ensemble model \n",
    "# to the ensemble model included as a non-trainable part of the clustering model\n",
    "# Note: Clustering model has  layers previous to passing on the information to the ensemble\n",
    "# For model_ensemble we have to skip the (nonexisting) input-layer's weights\n",
    "def cluster_model_transfer_weights(cluster_model, model_ensemble, N_layer_start = 10, N_layer_compete = 6, \n",
    "                                   n_clusters = 100, epsilon = 0.1, ensemble_option = True):\n",
    "    \n",
    "    # Check for comformity\n",
    "    if (len(cluster_model.layers) != len(model_ensemble.layers)+N_layer_start-1)&ensemble_option:\n",
    "        print('Models have varying layer configurations and are not compatible for this method.')\n",
    "        return\n",
    "    \n",
    "    if ensemble_option:\n",
    "        # Import weights of Ensemble Model\n",
    "        for i in range(len(model_ensemble.layers)-1):\n",
    "            cluster_model.layers[N_layer_start+i].set_weights(model_ensemble.layers[i+1].get_weights())\n",
    "            # Pretrained Ensemble Section non-trainable\n",
    "            cluster_model.layers[N_layer_start+i].trainable = False\n",
    "        \n",
    "    # Set weights for Competitive RNN Model\n",
    "    # Set 1: Pass on Information from previous layer (without changes)\n",
    "    W_pass = np.eye(N=n_clusters,M= n_clusters)\n",
    "    # Set 2: Competing Weights (i.e. effect of previous value)\n",
    "    W_compete = np.full(shape = (n_clusters,n_clusters), fill_value= -epsilon)\n",
    "    np.fill_diagonal(W_compete,val = 1)\n",
    "    cluster_model.layers[N_layer_compete].set_weights([W_pass, W_compete])\n",
    "    # Competitive Layer non-trainable\n",
    "    cluster_model.layers[N_layer_compete].trainable = False\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function which compares (or visualizes) agglomeration\n",
    "## We want to compare the baseline kMeans with the ANN Agglomeration\n",
    "## For the ANN we have to consider the predicted reserve (by part I), which\n",
    "# is integrated in the ANN-Model for clustering\n",
    "# and compare this to the reserve based on classical calculation and the representatives\n",
    "# obtain by the ANN-Cluster-Model\n",
    "# option_plot_selection: insert a a list of numbers, representing the selcted clusters to be plotted\n",
    "\n",
    "def analyze_agglomeration(baseline, y, Max_min, interest_rate, x = None, include_ann = False,\n",
    "                          ann_representatives = None, ann_prediction = None, \n",
    "                          option = 'plot', individual_clusters = False, option_cluster_fit = True,\n",
    "                          n_columns = 5, figsize = (20,30), option_plot_selection = None):\n",
    "    \n",
    "    #Number of clusters\n",
    "    n_cl = baseline.cluster_centers_.shape[0]\n",
    "    # Number of members per cluster\n",
    "    count = kmeans_counts(baseline.labels_,n_cl)\n",
    "    # size of output (vector of policy values)\n",
    "    n_out = targets.shape[1]\n",
    "    \n",
    "    # Round re-Transformed representatives (duration and aoc needs to be integer valued!!)\n",
    "    # Baseline\n",
    "    approx_low = np.floor(data_re_transform_features(baseline.cluster_centers_, Max_min, \n",
    "                                                     option= 'conditional')).astype('int')\n",
    "    approx_up = np.ceil(data_re_transform_features(baseline.cluster_centers_, Max_min, \n",
    "                                                   option= 'conditional')).astype('int')\n",
    "    if include_ann == True:\n",
    "        ann_approx_up = np.ceil(data_re_transform_features(ann_representatives, \n",
    "                                                        Max_min, option= 'conditional')).astype('int')\n",
    "        ann_approx_low = np.floor(data_re_transform_features(ann_representatives, \n",
    "                                                        Max_min, option= 'conditional')).astype('int')\n",
    "    \n",
    "    # Calculate targets for (floored and ceiled) centroids for baseline and\n",
    "    # optionally: also for representatives obtained by ANN\n",
    "    PV_low = np.zeros(shape = (n_cl, n_out))\n",
    "    PV_up = np.zeros(shape = (n_cl, n_out))\n",
    "    ann_PV_up = np.zeros(shape = (n_cl, n_out))\n",
    "    ann_PV_low = np.zeros(shape = (n_cl, n_out))\n",
    "    for i in range(n_cl):\n",
    "        \n",
    "        PV_low[i,0:max(approx_low[i,2]-approx_low[i,3],0)+1] = get_termlife_reserve_profile(\n",
    "                                                                age_curr = approx_low[i,0], \n",
    "                                                                Sum_ins= approx_low[i,1],\n",
    "                                                                 duration = approx_low[i,2], \n",
    "                                                                interest = interest_rate, \n",
    "                                                                 age_of_contract = approx_low[i,3], \n",
    "                                                                  option_past = False)\n",
    "        \n",
    "        PV_up[i,0:max(approx_up[i,2]-approx_up[i,3],0)+1] = get_termlife_reserve_profile(\n",
    "                                                                age_curr = approx_up[i,0], \n",
    "                                                                Sum_ins= approx_up[i,1],\n",
    "                                                                 duration = approx_up[i,2], \n",
    "                                                                interest = interest_rate, \n",
    "                                                                 age_of_contract = approx_up[i,3], \n",
    "                                                                  option_past = False)\n",
    "        # Optional: ALso for representatives of ANN\n",
    "        if include_ann == True:\n",
    "            dur_up = ann_approx_up[i,2]\n",
    "            aoc_up = ann_approx_up[i,3]\n",
    "            dur_low = ann_approx_low[i,2]\n",
    "            aoc_low = ann_approx_low[i,3]\n",
    "            ann_PV_up[i,0:dur_up-aoc_up+1] = get_termlife_reserve_profile(age_curr=ann_approx_up[i,0], \n",
    "                                                                 Sum_ins= ann_approx_up[i,1], \n",
    "                                                                 duration=dur_up, \n",
    "                                                                 interest= interest_rate, \n",
    "                                                                 age_of_contract= aoc_up,option_past= False)\n",
    "            ann_PV_low[i,0:dur_low-aoc_low+1] = get_termlife_reserve_profile(age_curr=ann_approx_low[i,0], \n",
    "                                                                 Sum_ins= ann_approx_low[i,1], \n",
    "                                                                 duration=dur_low, \n",
    "                                                                 interest= interest_rate, \n",
    "                                                                 age_of_contract= aoc_low,option_past= False)    \n",
    "\n",
    "\n",
    "    # Calculate actual targets per cluster \n",
    "    targets_cl = np.zeros(shape = (n_cl, n_output))\n",
    "    for i in range(n_cl):\n",
    "        index = baseline.labels_ == i\n",
    "        targets_cl[i,:] = targets[index,:].sum(axis=0)/count[i]\n",
    "\n",
    " \n",
    "    \n",
    "    if option == 'plot':\n",
    "        \n",
    "        if individual_clusters == True:\n",
    "            \n",
    "            \n",
    "            if option_plot_selection == None: # Plot all clusters C_1,..,C_K\n",
    "                fig, ax = plt.subplots(nrows = np.ceil(n_cl/n_columns).astype('int'), \n",
    "                                       ncols = n_columns, figsize = figsize)\n",
    "                ax = ax.flatten()\n",
    "\n",
    "                for i in range(n_cl):\n",
    "                    # Actual Targets\n",
    "                    ax[i].plot(targets_cl[i,:], 'r*', label = '$R(P)$')\n",
    "                    # Reserve based on K-Means clustering\n",
    "                    ax[i].plot(PV_up[i,:], linestyle = ':', color = 'grey', \n",
    "                               label = r'$ R( \\lceil\\tilde{P}_0\\rceil), R( \\lfloor\\tilde{P}_0\\rfloor)  $')\n",
    "                    ax[i].plot(PV_low[i,:], linestyle = ':', color = 'grey')\n",
    "\n",
    "                    if i%n_columns==0: # first column\n",
    "                        ax[i].set_ylabel('Policy Value', fontsize = 'large')\n",
    "                    if i>= (n_columns*(np.ceil(n_cl/n_columns).astype('int')-1)): # last row\n",
    "                        ax[i].set_xlabel('Time, $t$', fontsize = 'large')\n",
    "\n",
    "                    if include_ann == True:\n",
    "                        # Predicted Reserve by ANN\n",
    "                        ax[i].plot(ann_prediction[i,:], color = 'blue', label = r'$\\hat{f}_\\mathcal{N}(\\tilde{P}_{\\mathcal{N}})$')\n",
    "                        # Reserve based on classical calculation using representative contracts of ANN approach\n",
    "                        ax[i].plot(ann_PV_up[i,:], color = 'orange', \n",
    "                                   label = r'$ R( \\lceil\\tilde{P}_\\mathcal{N}\\rceil), R( \\lfloor\\tilde{P}_\\mathcal{N}\\rfloor)  $')\n",
    "                        ax[i].plot(ann_PV_low[i,:], color = 'orange') \n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[i].legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "            else: ### Plot only selection of clusters C_1,...,C_[option_plot_selection]\n",
    "                fig, ax = plt.subplots(nrows = np.ceil(len(option_plot_selection)/n_columns).astype('int'), \n",
    "                                       ncols = n_columns, figsize = figsize)\n",
    "                ax = ax.flatten()\n",
    "\n",
    "                for i in range(len(option_plot_selection)):\n",
    "                    # Actual Targets\n",
    "                    ax[i].plot(targets_cl[option_plot_selection[i],:], 'r*', label = '$R(P)$')\n",
    "                    # Reserve based on K-Means clustering\n",
    "                    ax[i].plot(PV_up[option_plot_selection[i],:], linestyle = ':', color = 'grey', \n",
    "                               label = r'$ R( \\lceil\\tilde{P}_0\\rceil), R( \\lfloor\\tilde{P}_0\\rfloor)  $')\n",
    "                    ax[i].plot(PV_low[option_plot_selection[i],:], linestyle = ':', color = 'grey')\n",
    "\n",
    "                    if i%n_columns==0: # first column\n",
    "                        ax[i].set_ylabel('Policy Value', fontsize = 'large')\n",
    "                    if i>= (len(option_plot_selection)-n_columns): # last row\n",
    "                        ax[i].set_xlabel('Time, $t$', fontsize = 'large')\n",
    "\n",
    "                    if include_ann == True:\n",
    "                        # Predicted Reserve by ANN\n",
    "                        ax[i].plot(ann_prediction[option_plot_selection[i],:], \n",
    "                                                          color = 'blue', label = r'$\\hat{f}_\\mathcal{N}(\\tilde{P}_{\\mathcal{N}})$')\n",
    "                        # Reserve based on classical calculation using representative contracts of ANN approach\n",
    "                        ax[i].plot(ann_PV_up[option_plot_selection[i],:], color = 'orange', \n",
    "                                   label = r'$ R( \\lceil\\tilde{P}_\\mathcal{N}\\rceil), R( \\lfloor\\tilde{P}_\\mathcal{N}\\rfloor)  $')\n",
    "                        ax[i].plot(ann_PV_low[option_plot_selection[i],:], color = 'orange') \n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[i].legend()\n",
    "                plt.tight_layout()\n",
    "            \n",
    "        \n",
    "        ### Look at cumulative cluster fit ###\n",
    "        fig_cum, ax_cum = plt.subplots(1,1)\n",
    "        ax_cum.plot((targets_cl*count).sum(axis=0), 'r*', label = '$R(P)$') # not scaled by numbers\n",
    "        ax_cum.plot((PV_up*count).sum(axis=0), color = 'grey', linestyle = ':', \n",
    "                 label =r'$ R( \\lceil\\tilde{P}_0\\rceil), R( \\lfloor\\tilde{P}_0\\rfloor)  $')\n",
    "        ax_cum.plot((PV_low*count).sum(axis=0), color = 'grey', linestyle = ':')\n",
    "        if include_ann == True:\n",
    "            ax_cum.plot((ann_prediction*count).sum(axis=0), label = r'$\\hat{f}_\\mathcal{N}(\\tilde{P}_{\\mathcal{N}})$')\n",
    "            ax_cum.plot((ann_PV_up*count).sum(axis = 0), color = 'orange', label = r'$ R( \\lceil\\tilde{P}_\\mathcal{N}\\rceil), R( \\lfloor\\tilde{P}_\\mathcal{N}\\rfloor)  $')\n",
    "            ax_cum.plot((ann_PV_low*count).sum(axis = 0), color = 'orange')\n",
    "        ax_cum.set_xlabel('Time, $t$', fontsize = 'large')\n",
    "        ax_cum.set_ylabel('Policy Value', fontsize = 'large')\n",
    "        ax_cum.set_title('$K=$ '+str(n_cl), fontsize= 'large')\n",
    "        ax_cum.legend() \n",
    "        \n",
    "    elif option == 'statistic':\n",
    "        # Compare target policy values with policy values of ANN representatives\n",
    "        # For the PV of ANN representatives we take the mean of the floored (ann_PV_low)\n",
    "        # and ceiled value (ann_PV_up) as our proxy\n",
    "        \n",
    "        df = pd.DataFrame(data=None, index = None, columns = ['min re${}_t$','mean re${}_t$',\n",
    "                                                              'max re${}_t$',r'$D$'])\n",
    "        \n",
    "        ## Cumulative\n",
    "        index_cum = ((targets_cl*count).sum(axis=0)>0)\n",
    "        \n",
    "        # Portfolio: K-Means\n",
    "        diff_km = (((PV_up*count+PV_low*count)/2).sum(axis=0)-(targets_cl*count).sum(axis=0))\n",
    "        diff_km_rel = diff_km[index_cum]/(targets_cl*count).sum(axis=0)[index_cum]\n",
    "        vol_km_discr = ((PV_up+PV_low)/2*count).sum()/(targets_cl*count).sum()-1\n",
    "        df.loc[r'$\\hat{R}(\\tilde{P}_0)$'] = (diff_km_rel.min(), diff_km_rel.mean(), diff_km_rel.max(), vol_km_discr)\n",
    "        \n",
    "        # Portfolio: ANN via classical calculation\n",
    "        diff = (((ann_PV_up*count+ann_PV_low*count)/2).sum(axis=0)-(targets_cl*count).sum(axis=0))\n",
    "        diff_rel = diff[index_cum]/(targets_cl*count).sum(axis=0)[index_cum]\n",
    "        vol_discr = ((ann_PV_up+ann_PV_low)/2*count).sum()/(targets_cl*count).sum()-1\n",
    "        df.loc[r'$\\hat{R}(\\tilde{P}_{\\mathcal{N}})$'] = (diff_rel.min(), diff_rel.mean(), diff_rel.max(), vol_discr)\n",
    "        \n",
    "        # Portfolio: ANN prediction\n",
    "        diff_pred = (ann_prediction*count).sum(axis=0)-(targets_cl*count).sum(axis=0)\n",
    "        diff_pred_rel = diff_pred[index_cum]/(targets_cl*count).sum(axis=0)[index_cum]\n",
    "        vol_pred_discr = (ann_prediction*count).sum()/(targets_cl*count).sum()-1\n",
    "        df.loc[r'$\\hat{f}_\\mathcal{N}(\\tilde{P}_{\\mathcal{N}})$'] = (diff_pred_rel.min(), diff_pred_rel.mean(), \n",
    "                                                                     diff_pred_rel.max(), vol_pred_discr)\n",
    "        \n",
    "        if option_cluster_fit == True:\n",
    "            \n",
    "            fig,ax = plt.subplots(1,3, figsize = figsize)\n",
    "            \n",
    "            \n",
    "            ##### Plot: ANN - Classical Calculation ######\n",
    "            values_ann = np.zeros(shape = (n_cl, 2))\n",
    "            # maximum policy value of representatives of clusters\n",
    "            values_ann[:,0] = targets_cl.mean(axis=1)\n",
    "            # total discrepancies (of cumulative volume) per cluster\n",
    "            values_ann[:,1] = ((ann_PV_up+ann_PV_low)/2*count).sum(axis=1)/(targets_cl*count).sum(axis=1)-1\n",
    "            \n",
    "            \n",
    "            #sort values for later rolling average mean\n",
    "            values_ann = values_ann[values_ann[:,0].argsort(),:]\n",
    "            \n",
    "            ax[0].axhline(y=0, xmax = values_ann[:,0].max(), linestyle = '--', color = 'grey', alpha = 0.2)\n",
    "            ax[0].plot(values_ann[:,0],np.cumsum(a=values_ann[:,1])/range(1,(n_cl+1)), color = 'red', \n",
    "                     linestyle = '--', label = 'Rolling Average')\n",
    "            ax[0].scatter(values_ann[:,0], values_ann[:,1], label = 'Cluster')\n",
    "            ax[0].set_xlabel('Mean PV of Cluster', fontsize = 'large')\n",
    "            ax[0].set_ylabel(r'Discrepancy, $D_\\hat{R}$', fontsize = 'large')\n",
    "            ax[0].set_title(r'ANN ($\\tilde{P}_\\mathcal{N}$) ', fontsize = 'large')\n",
    "            ax[0].legend()\n",
    "            \n",
    "            ######## Plot: ANN - Prediction ######\n",
    "            values_ann_pred = np.zeros(shape = (n_cl, 2))\n",
    "            # maximum policy value of representatives of clusters\n",
    "            values_ann_pred[:,0] = targets_cl.mean(axis=1)\n",
    "            # total discrepancies (of cumulative volume) per cluster\n",
    "            values_ann_pred[:,1] = (ann_prediction*count).sum(axis=1)/(targets_cl*count).sum(axis=1)-1\n",
    "            \n",
    "            \n",
    "            #sort values for later rolling average mean\n",
    "            values_ann_pred = values_ann_pred[values_ann_pred[:,0].argsort(),:]\n",
    "            \n",
    "            ax[1].axhline(y=0, xmax = values_ann_pred[:,0].max(), linestyle = '--', color = 'grey', alpha = 0.2)\n",
    "            ax[1].plot(values_ann_pred[:,0],np.cumsum(a=values_ann_pred[:,1])/range(1,(n_cl+1)), color = 'red', \n",
    "                     linestyle = '--', label = 'Rolling Average')\n",
    "            ax[1].scatter(values_ann_pred[:,0], values_ann_pred[:,1], label = 'Cluster')\n",
    "            ax[1].set_ylabel('Discrepancy, $D_{\\hat{f}_\\mathcal{N}}$', fontsize = 'large')\n",
    "            ax[1].set_xlabel('Mean PV of Cluster', fontsize = 'large')\n",
    "            ax[1].set_title(r'ANN ($\\tilde{P}_\\mathcal{N}$)', fontsize = 'large')\n",
    "\n",
    "            \n",
    "            ### Plot: K-Means ###\n",
    "            values_km = np.zeros(shape = (n_cl, 2))\n",
    "            # maximum policy value of representatives of clusters\n",
    "            values_km[:,0] = targets_cl.mean(axis=1)\n",
    "            # total discrepancies (of cumulative volume) per cluster\n",
    "            values_km[:,1] = ((PV_up+PV_low)/2*count).sum(axis=1)/(targets_cl*count).sum(axis=1)-1\n",
    "            \n",
    "            # sort values for later time dependent mean\n",
    "            values_km = values_km[values_km[:,0].argsort(),:]\n",
    "            \n",
    "            ax[2].axhline(y=0, xmax = values_km[:,0].max(), linestyle = '--', color = 'grey', alpha = 0.2)\n",
    "            ax[2].plot(values_km[:,0],np.cumsum(a=values_km[:,1])/range(1,(n_cl+1)), color = 'red', \n",
    "                     linestyle = '--')#, label = 'Rolling Average')\n",
    "            ax[2].scatter(values_km[:,0], values_km[:,1])#, label = 'Cluster')\n",
    "            ax[2].set_xlabel('Mean PV of Cluster', fontsize = 'large')\n",
    "            ax[2].set_ylabel(r'Discrepancy, $D_\\hat{R}$', fontsize = 'large')\n",
    "            ax[2].set_title(r'$K$-Means ($\\tilde{P}_0$)', fontsize = 'large')\n",
    "            \n",
    "            #ax[2].legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "             \n",
    "            \n",
    "            \n",
    "        return (df, targets_cl, ann_PV_low,ann_PV_up, count)\n",
    "    else:\n",
    "        print('Unknown option!')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
