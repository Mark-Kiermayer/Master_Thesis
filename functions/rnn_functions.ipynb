{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.)  RNNS - Setup, Evaluation, Ensembling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function to set up a model\n",
    "\n",
    "def create_rnn_model(model_input,nodes=[41,41],  n_output=41, final_dense_layer = True, \n",
    "                     dense_act_fct = 'linear', act_fct_special = False,\n",
    "                     optimizer_type='adam', loss_type='mse', metric_type='mae', dropout_option=True, \n",
    "                     dropout_share=[0.2,0.2], lambda_layer = True, lambda_scale =1, log_scale=False, \n",
    "                     model_compile = True, return_option = 'model', branch_name = '', input_type = '3D'): #, init_style):\n",
    "    \n",
    "    n_layers = len(nodes)\n",
    "    \n",
    "    if (n_layers > len(dropout_share)) & dropout_option:\n",
    "        print('No. of dropouts does not match depth of model!' )\n",
    "        return\n",
    "    \n",
    "    if n_layers>1:\n",
    "        if input_type == '3D':\n",
    "            # Input suitable for RNN Layer\n",
    "            x = CuDNNLSTM(units = nodes[0],return_sequences = True, \n",
    "                      name = 'Layer_{}1'.format(branch_name))(model_input)\n",
    "        elif input_type == '2D':\n",
    "            # Prepare Input to be suitable for RNN Layer\n",
    "            x = Repeat(n = n_output)(model_input)\n",
    "            x = CuDNNLSTM(units = nodes[0],return_sequences = True, \n",
    "                      name = 'Layer_{}1'.format(branch_name))(x)\n",
    "        # Optional Dropout\n",
    "        if dropout_option == True: \n",
    "            if dropout_share[0]>0:\n",
    "                x = Dropout(dropout_share[0], name = 'Drop_{}{}_{}'.format(branch_name,1,dropout_share[0]))(x)\n",
    "        for i in range(1,n_layers-1):\n",
    "            x = CuDNNLSTM(units = nodes[i], return_sequences = True, name = 'Layer_'+str(i+1))(x)\n",
    "            # Optional Dropout\n",
    "            if dropout_option == True:\n",
    "                if dropout_share[i]>0:\n",
    "                    x=Dropout(dropout_share[i], \n",
    "                              name = 'Drop_{}{}_{}'.format(branch_name,i+1,dropout_share[i]))(x)\n",
    "                    \n",
    "        # Insert Last LSTM Layer manually and set return_sequences = False\n",
    "        x=CuDNNLSTM(units=nodes[n_layers-1],return_sequences = False, \n",
    "                    name = 'Layer_{}'.format(branch_name)+str(n_layers))(x)              \n",
    "    else:\n",
    "        if input_type == '3D':\n",
    "            # Input suitable for RNN Layer\n",
    "            x = CuDNNLSTM(units = nodes[0],return_sequences = False, \n",
    "                      name = 'Layer_{}1'.format(branch_name))(model_input)\n",
    "        elif input_type == '2D':\n",
    "            # Prepare Input to be suitable for RNN Layer\n",
    "            x = RepeatVector(n = n_output)(model_input)\n",
    "            x = CuDNNLSTM(units = nodes[0],return_sequences = False, \n",
    "                      name = 'Layer_{}1'.format(branch_name))(x)\n",
    "        else:\n",
    "            print('Unknown input_type!')\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    if final_dense_layer:\n",
    "        if dropout_option == True:\n",
    "            if dropout_share[n_layers-1]>0:\n",
    "                x=Dropout(dropout_share[n_layers-1], \n",
    "                          name = 'Drop_{}{}_{}'.format(branch_name,n_layers, dropout_share[n_layers-1]))(x)\n",
    "        # Final Dense Layer\n",
    "        if act_fct_special:\n",
    "            x = Dense(n_output, name = 'Layer_{}'.format(branch_name)+str(len(nodes)+1))(x)\n",
    "            x = dense_act_fct(x)\n",
    "        else:\n",
    "            x = Dense(n_output, name = 'Layer_{}'.format(branch_name)+str(len(nodes)+1))(x)\n",
    "            if dense_act_fct != 'linear':\n",
    "                x = Activation(activation = dense_act_fct, name = dense_act_fct + branch_name)(x)\n",
    "                \n",
    "    \n",
    "    if lambda_layer:\n",
    "        if log_scale:\n",
    "            x = Lambda(lambda x_var: tf.exp((x_var+1)/2*np.log(1+lambda_scale))-1, \n",
    "                       name = 'Log_Scaling_Layer{}'.format(branch_name))(x)\n",
    "        else:\n",
    "            x = Lambda(lambda x_var: (x_var+1)/2*lambda_scale, name = 'Scaling_Layer{}'.format(branch_name))(x)    \n",
    "    \n",
    "    #model.summary()\n",
    "    if return_option == 'model':\n",
    "        # Model Configuration\n",
    "        model = Model(inputs=model_input, outputs=x)\n",
    "        # Compile model\n",
    "        if model_compile: \n",
    "            model.compile(loss = loss_type, optimizer = optimizer_type, metrics = [metric_type] )\n",
    "        return model\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple models of the same type\n",
    "# Relevant for creating an ensemble model\n",
    "\n",
    "\n",
    "def create_multiple_rnn_models(number, model_input,nodes=[41,41],  n_output=41, final_dense_layer = True, \n",
    "                               dense_act_fct = 'linear', optimizer_type='adam', loss_type='mse', \n",
    "                               metric_type='mae', dropout_option=True, dropout_share=[0.2,0.2], \n",
    "                               lambda_layer = True, lambda_scale =1, log_scale=False, model_compile = True, \n",
    "                               return_option = 'model', branch_name = ''):\n",
    "    \n",
    "    models = []\n",
    "    for i in range(number):\n",
    "        models.append(create_rnn_model(model_input = model_input, nodes=nodes, n_output=n_output, \n",
    "                                       final_dense_layer = final_dense_layer, dense_act_fct = dense_act_fct, \n",
    "                                       optimizer_type=optimizer_type, loss_type=loss_type, \n",
    "                                       metric_type=metric_type, dropout_option=dropout_option, \n",
    "                                       dropout_share=dropout_share, lambda_layer = lambda_layer, \n",
    "                                       lambda_scale =lambda_scale, log_scale=log_scale, \n",
    "                                       model_compile = model_compile, return_option = return_option, \n",
    "                                       branch_name = str(i)))\n",
    "    # depending on the return_option used in create_rnn_model, models can be a Tensor output or a compiled model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: list of models\n",
    "# Output: list of their weight configurations\n",
    "# Purpose: Use list of weigts to transfer them to a ensemble model\n",
    "\n",
    "def multiple_models_get_weights(models_lst):\n",
    "    w = []\n",
    "    for model in models_lst:\n",
    "        w.append(model.get_weights())\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models in a list of models\n",
    "\n",
    "def train_individual_ensembles( models_lst, x_train, y_train, n_batch = 100, n_epochs = 20, val_share = 0.25, \n",
    "                               es_patience = 15, \n",
    "                               path = 'C:\\\\Users\\\\mark.kiermayer\\\\Documents\\\\Python Scripts\\\\Master Thesis - Code\\\\checkpoints\\\\Ensembles'):\n",
    "    \n",
    "    n_ensembles = len(models_lst)   \n",
    "    model_collection = []\n",
    "    hist = []\n",
    "    \n",
    "    # patient early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=es_patience)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(n_ensembles):\n",
    "        print('Training Model {} of '.format(i+1)+str(n_ensembles))\n",
    "        t = time.time()\n",
    "\n",
    "        # save only best configuration\n",
    "        mc = ModelCheckpoint(filepath = path+r'\\model_{}.h5'.format(i), monitor='val_loss', mode='min', \n",
    "                         verbose=0, save_best_only=True)\n",
    "        hist.append(models_lst[i].fit(x_train, y_train,batch_size = n_batch, epochs = n_epochs, \n",
    "                                 validation_split = val_share, verbose=0, callbacks= [es, mc] ).history)\n",
    "        # Save history\n",
    "        with open(path+'\\model_{}_hist.json'.format(i), 'w') as f:\n",
    "            json.dump(hist[i], f )\n",
    "        \n",
    "        #model_collection.append(model)\n",
    "        print('END of Model {}'.format(i+1)+'. Time passed: ' + str(int((time.time()-t)*100)/100) + ' sec.')\n",
    "    return models_lst, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that predicts, if a contract is still active (output 1) or matured ( output 0 )\n",
    "# The magnitude of the reserve is irrelevant\n",
    "\n",
    "def create_model_qualitative(INPUT, n_output = 41, opt='adam',loss_type= 'binary_crossentropy', \n",
    "                             metrics_type = 'accuracy', return_option = 'model', model_type = 'standard',\n",
    "                             branch_name = '', input_features = 'partial', input_type = '3D', \n",
    "                             act_fct_final = 'sigmoid', option_trainable = True):\n",
    "    \n",
    "    # Criterion, whether input input has appropriate features (option 'partial' -> duration, age of contract) or \n",
    "    # too many (option 'all' -> age, sum insured, duration, age of contract)\n",
    "    if input_features =='partial':\n",
    "        INPUT_model = INPUT\n",
    "    elif input_features == 'all':\n",
    "        INPUT_model = Lambda(lambda x_val: x_val[:,:,2:4])(INPUT)\n",
    "    else:\n",
    "        print('Unknown input_features.')\n",
    "        pass\n",
    "    \n",
    "    # Criterion if INPUT suitable for RNN (3D) or required additional preparation (2D)\n",
    "    if input_type == '2D':\n",
    "        INPUT_model = RepeatVector(n = n_output)(INPUT_model)\n",
    "    #else:\n",
    "        #do nothing\n",
    "        \n",
    "    \n",
    "    # 'standard' equals the prepared data (for RNN-usage), where we set a default value for time points \n",
    "    # where the contract has matured\n",
    "    if model_type == 'standard':\n",
    "        x = Dense(units=1,activation = 'sigmoid', name = 'Layer_Qual_{}1'.format(branch_name))(INPUT_model)\n",
    "        OUTPUT = Flatten(name='Transform_{}2'.format(branch_name))(x)\n",
    "    # 'plain' equals a plain, repretitive input of the scaled data, i.e. features are repeated n_output-times\n",
    "    elif model_type == 'plain':\n",
    "        x = Dense(units=1)(INPUT_model)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(units=n_output, activation = 'tanh')(x)\n",
    "        x = Reshape(target_shape=(n_output,1))(x)\n",
    "        x = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "        OUTPUT = Flatten()(x)\n",
    "        \n",
    "    # 'plain_extended' has the same input as 'plain', but a wider layer, i.e. more neurons.\n",
    "    elif model_type == 'plain_extended':\n",
    "        x = Dense(units=1)(INPUT_model)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(units=n_output, activation = 'tanh')(x)\n",
    "        OUTPUT = Dense(units = n_output, activation = 'sigmoid')(x)\n",
    "    else:\n",
    "        print('Unknown model_type.')\n",
    "        pass\n",
    "      \n",
    "    if return_option == 'model':\n",
    "        model = Model(inputs = INPUT, outputs = OUTPUT)\n",
    "        if model_type != 'standard':\n",
    "            # Use Bias to distinguish different timepoints of policy values\n",
    "            dummy = (input_features == 'all')+(input_type == '2D')\n",
    "            model.layers[3+dummy].set_weights([np.eye(n_output, n_output), -np.linspace(0,1,n_output)])\n",
    "            model.layers[3+dummy].trainable = False\n",
    "            \n",
    "        # Check if model return should be trainable\n",
    "        if option_trainable == False:\n",
    "            for l in model.layers:\n",
    "                l.trainable = False\n",
    "        model.compile(optimizer=opt,loss=loss_type,metrics= [metrics_type])\n",
    "        \n",
    "        \n",
    "        \n",
    "        return model\n",
    "    elif return_option == 'output':\n",
    "        return OUTPUT\n",
    "    else:\n",
    "        print('Unknown return_option.')\n",
    "        return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a model that predicts over the zero-padded time-vector, if a contract is still active (output 1) \n",
    "# or matured ( output 0 )\n",
    "\n",
    "def create_model_qualitative_test(INPUT, n_out = 41, opt='adam',loss_type= 'binary_crossentropy', \n",
    "                             metrics_type = 'accuracy', return_option = 'model', \n",
    "                             branch_name = ''):\n",
    "    # Aim: Model for determining if contract is reserve is required (magnitude not relevant)\n",
    "    \n",
    "    # Time points as Artificial Input. Aim: Use them to distinguish the repetitive, constant input\n",
    "    #constants = list(range(n_out))\n",
    "    #fixed_input = Input(tensor = K.variable(constants))\n",
    "    x = Dense(units=1, activation = 'sigmoid', name = 'Layer_Qual_{}1'.format(branch_name))(INPUT)\n",
    "    x = Flatten()(x)\n",
    "    #x = Lambda(lambda x_val: x_val - K.variable(constants))(x)\n",
    "    x = Lambda(lambda x_val: x_val - np.linspace(0,1,n_out))(x)\n",
    "    # Reverse Flattening\n",
    "    x = Lambda(lambda x_val: tf.reshape(x_val, [_,n_out, 1]))(x)\n",
    "    OUTPUT = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    #OUTPUT = subtract([x,fixed_input])\n",
    "    #OUTPUT = Dense(units=n_out,activation = 'relu', name = 'Layer_Qual_{}2'.format(branch_name))(x)\n",
    "    \n",
    "    #OUTPUT = Flatten(name='Transform_{}'.format(branch_name))(x)\n",
    "    \n",
    "     \n",
    "    if return_option == 'model':\n",
    "        model = Model(inputs = [INPUT], outputs = OUTPUT)\n",
    "        model.compile(optimizer=opt,loss=loss_type,metrics= [metrics_type])\n",
    "        return model\n",
    "    elif return_option == 'output':\n",
    "        return OUTPUT\n",
    "    else:\n",
    "        print('Unknown return_option.')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with integrated (non-trainable) qualitative part\n",
    "# Advantage: Training of Quantative Part can be focused on timepoints where contract is still active\n",
    "def create_quant_qual_model(model_input,nodes=[41],  n_output=41, weights_qualitative = None, position_qualitative = 5,\n",
    "                            final_dense_layer = True, \n",
    "                             dense_act_fct = 'tanh', act_fct_special = False,\n",
    "                             optimizer_type='adam', loss_type='mse', metric_type='mae', dropout_option=False, \n",
    "                             lambda_layer = True, lambda_scale =1, log_scale=True, branch_name = ''):\n",
    "    \n",
    "    x = RepeatVector(n_output)(model_input)\n",
    "    quant = create_rnn_model(model_input=x,nodes=nodes,  n_output=n_output, final_dense_layer = final_dense_layer, \n",
    "                     dense_act_fct = dense_act_fct, act_fct_special = act_fct_special,\n",
    "                     optimizer_type=optimizer_type, loss_type=loss_type, metric_type=metric_type, \n",
    "                         dropout_option=dropout_option, lambda_layer = lambda_layer, lambda_scale =lambda_scale, \n",
    "                         log_scale=log_scale, return_option = 'output', branch_name = branch_name,\n",
    "                         input_type = '3D')\n",
    "    # Slice input for qualitative model\n",
    "    y = Lambda(lambda x_val: x_val[:,:,2:4], name = 'Slicing_'+branch_name)(x)\n",
    "    # Include Qualitative Model\n",
    "    y = create_model_qualitative(y, return_option = 'output')\n",
    "    # Binary (non-trainable !) Transformation\n",
    "    qual = Lambda(lambda x_val: tf.round(x_val), name = 'Binary_Transformation')(y)\n",
    "    \n",
    "    # Combine quant.&qual Output\n",
    "    OUTPUT = multiply([quant, qual], name='Combine_Quantitative_Qualitative_'+branch_name)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=OUTPUT)     \n",
    "    \n",
    "    # Import qualitative model's weights\n",
    "    model.layers[position_qualitative].set_weights(weights_qualitative)\n",
    "    model.layers[position_qualitative].trainable = False\n",
    "    #Compile model\n",
    "    model.compile(loss = loss_type, optimizer = optimizer_type, metrics = [metric_type] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#######################\n",
    "old backup Version (before we included the model_qualitative_type option\n",
    "#######################\n",
    "\n",
    "\n",
    "## Combine various single-models to an ensembles model (optionally inclusively a qualitative model)\n",
    "## With load_weights, the pre-trained, single models' weights can be transferred to the ensemble model\n",
    "# Options for 'model_qualitative_type' are 'standard', 'plain', 'plain_extended'\n",
    "\n",
    "def combine_models(input_layer, n_ensembles =1, model_qualitative_option = False, model_qualitative_type = 'standard',\n",
    "                   load_weights = False, weights_ensembles = None, \n",
    "                   weights_qualitative = None, scale = 1, LSTM_nodes = [41,41], output_nodes = 41,\n",
    "                    final_dense_layer = True, dense_act_fct = 'linear', act_fct_special = False, return_option = 'model'):\n",
    "    \n",
    "    # Aim: Merge a quantitative Ensemble-Model with a Qualitative classification Model\n",
    "    \n",
    "    output_ensemble = []\n",
    "    # Create all Single Models for Ensemble\n",
    "    for i in range(n_ensembles):\n",
    "        model_ens =create_rnn_model(model_input=input_layer, nodes = LSTM_nodes, n_output = output_nodes,\n",
    "                                    final_dense_layer = final_dense_layer, dense_act_fct= dense_act_fct,\n",
    "                                    act_fct_special=act_fct_special,\n",
    "                                    dropout_option=False,dropout_share=[0.2,0.2],lambda_layer = True, \n",
    "                                    lambda_scale =scale, log_scale=True, branch_name = str(i), return_option= return_option)\n",
    "        \n",
    "        if return_option == 'model':\n",
    "        # Load weights; Not reasonable for non-model returns\n",
    "            if load_weights:\n",
    "                model_ens.set_weights(weights_ensembles[i])\n",
    "            # Save the single-models' outputs in a list -> will be used as input to Average-Layer\n",
    "            output_ensemble.append(model_ens.outputs[0])\n",
    "        else:\n",
    "            # For return_option 'output' ANN-objective .outputs does not exist\n",
    "            output_ensemble.append(model_ens)\n",
    "    \n",
    "    # Combine Ensembles by Averaging them\n",
    "    output_av = Average(name = 'Ensembles_Combine')(output_ensemble)\n",
    "    \n",
    "    if model_qualitative_option:\n",
    "        input_qual = Lambda(lambda x: x[:,:,2:4])(input_layer)\n",
    "        model_qual = create_model_qualitative(INPUT = input_qual, return_option='output', \n",
    "                                              branch_name=str(n_ensembles), model_type= model_qualitative_type)\n",
    "        # For return_option 'output' ANN-objectives do not exist\n",
    "        output_qual = model_qual\n",
    "\n",
    "        #output_qual = Lambda(lambda x: tf.cond(x>0.5,1,0), name = 'Binary_Transformation')(output_qual)\n",
    "        output_qual = Lambda(lambda x: tf.round(x), name = 'Binary_Transformation')(output_qual)\n",
    "        OUTPUT = multiply([output_av, output_qual], name='Combine_Quantitative_Qualitative')\n",
    "    else:\n",
    "        OUTPUT = output_av\n",
    "\n",
    "    \n",
    "    if return_option == 'model':\n",
    "        model = Model(inputs = [input_layer], outputs = OUTPUT)\n",
    "        model.compile(optimizer='adam',loss='mse',metrics= ['mae'])\n",
    "        \n",
    "        # Set weights for Qualitative Model\n",
    "        if load_weights & model_qualitative_option:\n",
    "            \n",
    "            if model_qualitative_type == 'standard':\n",
    "                # Per single model in ensemble: variable_number x CuDNNLSTM, 1x Dense, 1xLambda\n",
    "                # Additionally, 1x input layer and 1x averaging layer\n",
    "                model.layers[(len(LSTM_nodes)+2)*n_ensembles+2].set_weights(weights_qualitative)\n",
    "                model.layers[(len(LSTM_nodes)+2)*n_ensembles+2].trainable = False\n",
    "            elif model_qualitative_type == 'plain':\n",
    "                # Weights have to be transfered at\n",
    "                # 1.) At position 3: 1st Dense Layer\n",
    "                # 2.) After: 4 preliminary Layers, 2 qual. layers (Dense with non-trainable weights and Reshape Layer)\n",
    "                #       and 3 (CuDNNLSTM, Dense and Activation Layer) x N_ensemble Layers\n",
    "                model.layers[2].set_weights(weights_qualitative)\n",
    "                model.layers[2].trainable = False\n",
    "        \n",
    "        return model\n",
    "    else:\n",
    "        return OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine various single-models to an ensembles model (optionally inclusively a qualitative model)\n",
    "## With load_weights, the pre-trained, single models' weights can be transferred to the ensemble model\n",
    "# Options for 'model_qualitative_type' are 'standard', 'plain', 'plain_extended'\n",
    "\n",
    "def combine_models(input_layer, n_ensembles =1, model_qualitative_option = False, model_qualitative_type = 'standard',\n",
    "                   load_weights = False, weights_ensembles = None, \n",
    "                   weights_qualitative = None, scale = 1, LSTM_nodes = [41,41], output_nodes = 41,\n",
    "                    final_dense_layer = True, dense_act_fct = 'linear', act_fct_special = False, return_option = 'model'):\n",
    "    \n",
    "    # Aim: Merge a quantitative Ensemble-Model with a Qualitative classification Model\n",
    "    \n",
    "    output_ensemble = []\n",
    "    # Create all Single Models for Ensemble\n",
    "    for i in range(n_ensembles):\n",
    "        model_ens =create_rnn_model(model_input=input_layer, nodes = LSTM_nodes, n_output = output_nodes,\n",
    "                                    final_dense_layer = final_dense_layer, dense_act_fct= dense_act_fct,\n",
    "                                    act_fct_special=act_fct_special,\n",
    "                                    dropout_option=False,dropout_share=[0.2,0.2],lambda_layer = True, \n",
    "                                    lambda_scale =scale, log_scale=True, branch_name = str(i), return_option= return_option)\n",
    "        \n",
    "        if return_option == 'model':\n",
    "        # Load weights; Not reasonable for non-model returns\n",
    "            if load_weights:\n",
    "                model_ens.set_weights(weights_ensembles[i])\n",
    "            # Save the single-models' outputs in a list -> will be used as input to Average-Layer\n",
    "            output_ensemble.append(model_ens.outputs[0])\n",
    "        else:\n",
    "            # For return_option 'output' ANN-objective .outputs does not exist\n",
    "            output_ensemble.append(model_ens)\n",
    "    \n",
    "    # Combine Ensembles by Averaging them\n",
    "    output_av = Average(name = 'Ensembles_Combine')(output_ensemble)\n",
    "    \n",
    "    if model_qualitative_option:\n",
    "        \n",
    "\n",
    "        model_qual = create_model_qualitative(INPUT = input_layer, return_option=return_option, \n",
    "                                              input_features= 'all', input_type= '3D', option_trainable= False,\n",
    "                                              branch_name=str(n_ensembles), model_type= model_qualitative_type)\n",
    "        if return_option == 'model':\n",
    "            # Set weights for Qualitative Model\n",
    "            if load_weights:\n",
    "                model_qual.set_weights(weights_qualitative)\n",
    "\n",
    "            # For return_option 'output' ANN-objectives do not exist\n",
    "            output_qual = model_qual.outputs[0]\n",
    "        elif return_option == 'output':\n",
    "            output_qual = model_qual\n",
    "        else:\n",
    "            print('Unknown return_option')\n",
    "            pass\n",
    "\n",
    "        #output_qual = Lambda(lambda x: tf.cond(x>0.5,1,0), name = 'Binary_Transformation')(output_qual)\n",
    "        output_qual = Lambda(lambda x: tf.round(x), name = 'Binary_Transformation')(output_qual)\n",
    "        OUTPUT = multiply([output_av, output_qual], name='Combine_Quantitative_Qualitative')\n",
    "    else:\n",
    "        OUTPUT = output_av\n",
    "\n",
    "    \n",
    "    if return_option == 'model':\n",
    "        model = Model(inputs = [input_layer], outputs = OUTPUT)\n",
    "        model.compile(optimizer='adam',loss='mse',metrics= ['mae'])\n",
    "        \n",
    "        return model\n",
    "    else:\n",
    "        return OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
